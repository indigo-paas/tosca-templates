tosca_definitions_version: tosca_simple_yaml_1_0

imports:
  - base_types: https://github.com/indigo-paas/tosca-types/raw/main/tosca_types/applications/dodas_types.yaml 

description: Deploy a DODAS cluster

metadata:
  display_name: DODAS cluster
  template_type: "kubernetes"

topology_template:

  inputs:

    admin_token:
      description: password token for accessing k8s dashboard and grafana dashboard
      type: string
      required: yes

    cluster_secret:
      description: token for HTCondor daemon-daemon authN
      type: string
      required: yes

    number_of_masters:
      description: number of VMs for K8s master
      type: integer
      required: no
      default: 1
      constraints:
        - valid_values: [ 1 ]

    frontend_dns_name:
      description: DNS name for the main k8s endpoints (api server, dashboard, monitoring)
      type: string
      required: yes

    dyn_dns_service_token:
      description: Copy here the secret generated by the Dynamic DNS service
      type: string
      required: yes

    num_cpus_master:
      description: number of CPU for K8s master VM
      type: integer
      default: 4

    mem_size_master:
      description: memory size for K8s master VM
      type: string
      default: "8 GB"

    number_of_nodes:
      description: number of K8s node VMs
      type: integer
      default: 4

    num_cpus_node:
      description: number of CPUs for K8s node VM
      type: integer
      default: 4

    mem_size_node:
      description: memory size for K8s node VM
      type: string
      default: "8 GB"

    ports:
      type: map
      required: false
      default: { "http": { "protocol": "tcp", "source": 80 }, "https": { "protocol": "tcp", "source": 443 }, "collector": { "protocol": "tcp", "source": 30618 }, "schedd": { "protocol": "tcp", "source": 31618 }}
      constraints:
        - min_length: 0
      entry_schema:
        type: tosca.datatypes.indigo.network.PortSpec
      description: Ports to open on the K8s master VM

    wn_image:
      description: HTCondor WN image name
      type: string
      default: "htcondor/execute"

    wn_tag:
      description: HTCondor WN tag name
      type: string
      default: "8.9.9-el7"

    iam_server:
      description: IAM server name for HTCondor authN
      type: string
      default: "dodas-iam.cloud.cnaf.infn.it"


  node_templates:

    longhorn:
      type: tosca.nodes.DODAS.HelmInstall
      properties:
        repos:
          - { name: longhorn, url: "https://charts.longhorn.io" }
        name: longhorn
        chart: "longhorn/longhorn"
        inline_options: "--version 1.3.0 -n longhorn-system --create-namespace"
        helm_version: v3
        values_file: |
          persistence:
            defaultClassReplicaCount: 1
      requirements:
        - host: k8s_master_server
        - dependency: k8s_master

    helm_install:
      type: tosca.nodes.DODAS.HelmInstall.HTCondor
      properties:
        name: "condor"
        chart: "dodas/htcondor"
        repos:
        - { name: dodas, url: "https://dodas-ts.github.io/helm_charts" }
        condor_host:  { get_input: frontend_dns_name }
        condor_public_ip: { get_attribute: [ k8s_master_server, public_address, 0 ] }
        wn_image: { get_input: wn_image }
        wn_tag: { get_input: wn_tag }
        iam_server: {get_input: iam_server }
        cluster_secret: {get_input: cluster_secret }
        helm_version: v3
        inline_options: "--version 3.0.4"
        values_file: |
          # Default values for htcondor.
          # This is a YAML-formatted file.
          # Declare variables to be passed into your templates.

          cluster:
            secret: {{ cluster_secret }}

          schedd:
            enabled: true
            mapfile: |
              SCITOKENS https:\\/\\/{{ iam_server }}\\/,(.*) \\1@users.htcondor.org
              PASSWORD (*.) condor
              GSI (.*) anonymous
            extraconfig: ''
            hostname: {{ condor_host }}
            service:
              type: NodePort
              nodePort: 31618
              targetPort: 31618
            image:
              name: {{ schedd_image }}
              tag:  '{{ schedd_tag }}'
              pullPolicy: IfNotPresent
            persistence:
              spooldir:
                enabled: true
                storageClass: longhorn
                size: 20Gi
            requests:
              memory: '500M'
              cpu: '100m'

          master:
            enabled: true
            publicIP: {{ condor_public_ip }}
            extraconfig: ''
            hostname: {{ condor_host }}
            service:
              type: NodePort
              nodePort: 30618
              targetPort: 30618
            image:
              name: {{ cm_image }}
              tag:  '{{ cm_tag }}'
              pullPolicy: IfNotPresent
            requests:
              memory: '500M'
              cpu: '100m'

          wn:
            enabled: true
            affinity: |
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                      - schedd
                      - master
                  topologyKey: kubernetes.io/hostname
            replicas: 1
            image:
              name:  {{ wn_image }}
              tag:  '{{ wn_tag }}'
              pullPolicy: IfNotPresent
            # Condor slot type
            slotType: cpus=1, mem=2000
            requests:
              memory: '1500M'
              cpu: 1
            # Define you own persistent volume to mount
            persistentVolume:
              pv:
                name: ''
                spec: ''
              pvc:
                name: ''
                mountPath: ''
                spec: ''

          htcClient:
            enabled: false
            image:
              name: htcondor/execute
              pullPolicy: IfNotPresent
              tag: '8.9.9-el7'

          # Enable Squid server
          squid:
            enabled: false

          # CVMFS mount on slave: configuration
          cvmfs:
            enabled: false
            image: dodasts/cvmfs
            tag: latest
            pullPolicy: IfNotPresent

            # List of repos to be mounted
            repoList: cms.cern.ch  grid.cern.ch  oasis.cern.ch  singularity.opensciencegrid.org

            privKey: []
            #  - name: spiga
            #    path: spiga.local.repo
            #    filename: spiga.local.repo.pub
            #    content: |
            #      test

            # default.local file content
            defaultLocalConfig: []
            #  - file: spiga.local.repo.conf
            #    content: |
            #      spiga.local.repo.conf: |-
            #        property.1=value-1
            #        property.2=value-2
      requirements:
       - host: k8s_master_server
       - dependency: longhorn

    jhub_install:
      type: tosca.nodes.DODAS.HelmInstall.JHub
      properties:
        name: "hub"
        chart: "dodas/jhubaas"
        repos:
        - { name: dodas, url: "https://dodas-ts.github.io/helm_charts" }
        condor_host:  {  get_input: frontend_dns_name }
        jlab_host: { get_input: frontend_dns_name }
        helm_version: v3
        inline_options: "--version 3.0.5"
        values_file: |
          hostname: {{ condor_host }}
          ingress: true
          # Show options for htcondor integration
          htcondor: true
          jupyterhub:
            ingress:
              # Keep default ingress disabled
              enabled: false

            cull:
              enabled: true
              timeout: 3600
              every: 600

            proxy:
              chp:
                networkPolicy:
                  enabled: false
              secretToken: 72077073d819ce7a118a7dd7e2ce3eb74328cd306faa0540b1045d56113fdd0e
              #<PUT HERE A RANDOM TOKEN openssl rand -hex 32>

            singleuser:
              networkPolicy:
                    enabled: false
              storage:
                homeMountPath: \"/opt/workspace/persistent_storage\"
                dynamic:
                  storageClass: longhorn

            hub:
              image:
                name: ghcr.io/dodas-ts/jhub
                tag: 'v5.0.1-pre15'
              cookieSecret: 72077073d819ce7a118a7dd7e2ce3eb74328cd306faa0540b1045d56113fdd0e
              networkPolicy:
                enabled: false
              baseUrl: /jlab/
              args:
              - jupyterhub
              - --config
              - /etc/jupyterhub/jupyterhub_config.py
              - --upgrade-db
              db:
                type: sqlite-pvc
                upgrade:
                pvc:
                  accessModes:
                    - ReadWriteOnce
                  storage: 1Gi
                  storageClassName: longhorn
              extraEnv:
                OAUTH_CALLBACK_URL: https://{{ jlab_host }}/jlab/oauth_callback
                OAUTH_ENDPOINT: https://dodas-iam.cloud.cnaf.infn.it/
                OAUTH_GROUPS: /users
                ADMIN_OAUTH_GROUPS: /admins
                WITH_GPU: \"false\"
                HTCONDOR_COLLECTOR_URL: {{ condor_host }}:30618
                HTCONDOR_SCHEDD_NAME: {{ condor_host }}
                ACCESS_TOKEN: \"\"
                SSH_NAMESPACE: default
              extraConfig:
                myConfig.py: |
                  #!/usr/bin/env python
                  # -*- coding: utf-8 -*-
                  import os
                  import socket
                  import json
                  #from oauthenticator.github import GitHubOAuthenticator
                  from oauthenticator.oauth2 import OAuthenticator
                  from oauthenticator.generic import GenericOAuthenticator
                  from tornado import gen
                  import kubespawner
                  import subprocess
                  import warnings

                  import os

                  from subprocess import check_call

                  c.JupyterHub.tornado_settings = {'max_body_size': 1048576000, 'max_buffer_size': 1048576000}
                  c.JupyterHub.log_level = 30

                  callback = os.environ[\"OAUTH_CALLBACK_URL\"]
                  os.environ[\"OAUTH_CALLBACK\"] = callback
                  iam_server = os.environ[\"OAUTH_ENDPOINT\"]

                  server_host = socket.gethostbyname(socket.getfqdn())
                  os.environ[\"IAM_INSTANCE\"] = iam_server

                  myenv = os.environ.copy()

                  c.Spawner.default_url = '/lab'

                  client_id = \"egi-demo\"
                  client_secret = \"fDS4tbwHr8I6J4XuV4pVhR9J-HclHdxJvfYbd-uth0BPOJdFeJBnvXnhlBqu1JA6zehKVBT9hd6AdIdDLHnyqw\"

                  class EnvAuthenticator(GenericOAuthenticator):

                      @gen.coroutine
                      def pre_spawn_start(self, user, spawner):
                          auth_state = yield user.get_auth_state()
                          import pprint
                          pprint.pprint(auth_state)
                          if not auth_state:
                              # user has no auth state
                              return
                          # define some environment variables from auth_state
                          self.log.info(auth_state)
                          spawner.environment['IAM_SERVER'] = iam_server
                          spawner.environment['IAM_CLIENT_ID'] = client_id
                          spawner.environment['IAM_CLIENT_SECRET'] = client_secret
                          spawner.environment['ACCESS_TOKEN'] = auth_state['access_token']
                          spawner.environment['REFRESH_TOKEN'] = auth_state['refresh_token']
                          spawner.environment['USERNAME'] = auth_state['oauth_user']['preferred_username']
                          spawner.environment['JUPYTERHUB_ACTIVITY_INTERVAL'] = \"15\"
                          spawner.environment['SSH_NAMESPACE'] = os.environ.get(\"SSH_NAMESPACE\") 

                          amIAllowed = False
                          
                          import jwt
                          groups = jwt.decode(auth_state[\"access_token\"], options={\"verify_signature\": False, \"verify_aud\": False})[\"wlcg.groups\"]
                          
                          if os.environ.get(\"OAUTH_GROUPS\"):
                              spawner.environment['GROUPS'] = \" \".join(groups)
                              allowed_groups = os.environ[\"OAUTH_GROUPS\"].split(\" \")
                              self.log.info(groups)
                              for gr in allowed_groups:
                                  if gr in groups:
                                      amIAllowed = True
                          else:
                              amIAllowed = True

                          if not amIAllowed:
                                  self.log.error(
                                      \"OAuth user contains not in group the allowed groups %s\" % allowed_groups
                                  )
                                  raise Exception(\"OAuth user not in the allowed groups %s\" % allowed_groups)

                      async def authenticate(self, handler, data=None):
                          code = handler.get_argument(\"code\")
                          # TODO: Configure the curl_httpclient for tornado
                          http_client = self.http_client()

                          params = dict(
                              redirect_uri=self.get_callback_url(handler),
                              code=code,
                              grant_type='authorization_code',
                          )
                          params.update(self.extra_params)

                          headers = self._get_headers()

                          token_resp_json = await self._get_token(http_client, headers, params)

                          user_data_resp_json = await self._get_user_data(http_client, token_resp_json)



                          if callable(self.username_key):
                              name = self.username_key(user_data_resp_json)
                          else:
                              name = user_data_resp_json.get(self.username_key)
                              if not name:
                                  self.log.error(
                                      \"OAuth user contains no key %s: %s\", self.username_key, user_data_resp_json
                                  )
                                  return

                          auth_state = self._create_auth_state(token_resp_json, user_data_resp_json)
                          import pprint
                          pprint.pprint(auth_state)
                          
                          import jwt
                          groups = jwt.decode(auth_state[\"access_token\"], options={\"verify_signature\": False, \"verify_aud\": False})[\"wlcg.groups\"]
                          
                          pprint.pprint(groups)
                          
                          is_admin = False
                          if os.environ.get(\"ADMIN_OAUTH_GROUPS\") in groups:
                              self.log.info(\"%s : %s is in %s\" , (name, os.environ.get(\"ADMIN_OAUTH_GROUPS\"), groups))
                              is_admin = True
                          else:
                              self.log.info(\" %s is not in admin group \", name)


                          return {
                              'name': name,
                              'admin': is_admin,
                              'auth_state': auth_state #self._create_auth_state(token_resp_json, user_data_resp_json)
                          }

                  #c.JupyterHub.authenticator_class = GitHubEnvAuthenticator
                  c.JupyterHub.authenticator_class = EnvAuthenticator
                  c.GenericOAuthenticator.oauth_callback_url = callback

                  # PUT IN SECRET
                  c.GenericOAuthenticator.client_id = client_id
                  c.GenericOAuthenticator.client_secret = client_secret
                  c.GenericOAuthenticator.authorize_url = iam_server.strip('/') + '/authorize'
                  c.GenericOAuthenticator.token_url = iam_server.strip('/') + '/token'
                  c.GenericOAuthenticator.userdata_url = iam_server.strip('/') + '/userinfo'
                  c.GenericOAuthenticator.scope = ['openid', 'profile', 'email', 'offline_access', 'wlcg', 'wlcg.groups']
                  c.GenericOAuthenticator.username_key = \"preferred_username\"

                  c.GenericOAuthenticator.enable_auth_state = True
                  if 'JUPYTERHUB_CRYPT_KEY' not in os.environ:
                      warnings.warn(
                          \"Need JUPYTERHUB_CRYPT_KEY env for persistent auth_state.\\n\"
                          \"    export JUPYTERHUB_CRYPT_KEY=$(openssl rand -hex 32)\"
                      )
                      c.CryptKeeper.keys = [ os.urandom(32) ]


                  class CustomSpawner(kubespawner.KubeSpawner):
                      def _options_form_default(self):
                          return \"\"\"
                          <label for=\"stack\">Select your desired image:</label>
                    <input list=\"images\" name=\"img\">
                    <datalist id=\"images\">
                      <option value=\"ghcr.io/dodas-ts/notebook:v5.0.1-pre15-base\">Notebook base</option>
                      <option value=\"ghcr.io/dodas-ts/notebook:v5.0.1-pre15-htc\">Notebook with HTCondor</option>
                      <option value=\"ghcr.io/dodas-ts/notebook:v5.0.1-pre15-htc-root\">Notebook with HTCondor and ROOT</option>
                    </datalist>
                  <br>
                          <label for=\"cpu\">Select your desired number of cores:</label>
                          <select name=\"cpu\" size=\"1\">
                    <option value=\"1\">1</option>
                    <option value=\"2\">2</option>
                  </select>
                  <br>
                          <label for=\"mem\">Select your desired memory size:</label>
                          <select name=\"mem\" size=\"1\">
                    <option value=\"2G\">2GB</option>
                    <option value=\"4G\">4GB</option>
                  </select>
                  <br>

                  \"\"\"

                      def options_from_form(self, formdata):
                          options = {}
                          options['img'] = formdata['img']
                          container_image = ''.join(formdata['img'])
                          print(\"SPAWN: \" + container_image + \" IMAGE\" )
                          self.image = container_image

                          options['cpu'] = formdata['cpu']
                          cpu = ''.join(formdata['cpu'])
                          self.cpu_guarantee = float(cpu)
                          self.cpu_limit = float(cpu)

                          options['mem'] = formdata['mem']
                          memory = ''.join(formdata['mem'])
                          self.mem_guarantee = memory
                          self.mem_limit = memory

                          #options['gpu'] = formdata['gpu']
                          #use_gpu = True if ''.join(formdata['gpu'])==\"Y\" else False
                          #if use_gpu:
                          #    self.extra_resource_guarantees = {\"nvidia.com/gpu\": \"1\"}
                          #    self.extra_resource_limit = {\"nvidia.com/gpu\": \"1\"}
                          return options

                  c.JupyterHub.spawner_class = CustomSpawner

                  c.KubeSpawner.cmd = [\"/usr/local/bin/jupyterhub-singleuser\"]
                  c.KubeSpawner.args = [\"--allow-root\"]
                  c.KubeSpawner.privileged = True

                  c.KubeSpawner.extra_pod_config = {
                      \"automountServiceAccountToken\": True,
                          }
                        
                  c.KubeSpawner.environment = {
                    \"_condor_COLLECTOR_HOST\": os.environ[\"HTCONDOR_COLLECTOR_URL\"],
                    \"_condor_SCHEDD_HOST\": os.environ[\"HTCONDOR_SCHEDD_NAME\"],
                    \"_condor_SCHEDD_NAME\": os.environ[\"HTCONDOR_SCHEDD_NAME\"],
                    \"_condor_AUTH_SSL_CLIENT_CAFILE\": \"/ca.crt\",
                    \"_condor_SEC_DEFAULT_AUTHENTICATION_METHODS\": \"SCITOKENS\",
                    \"_condor_SCITOKENS_FILE\": \"/tmp/token\",
                    \"_condor_TOOL_DEBUG\": \"D_FULLDEBUG,D_SECURITY\"
                  }

                  c.KubeSpawner.extra_container_config = {
                      \"securityContext\": {
                              \"privileged\": True,
                              \"capabilities\": {
                                          \"add\": [\"SYS_ADMIN\"]
                                      }
                          }
                  }
                  c.KubeSpawner.http_timeout = 600
                  c.KubeSpawner.start_timeout = 600

                  c.JupyterHub.hub_connect_ip = 'hub.default.svc.cluster.local'

                  c.KubeSpawner.notebook_dir = \"/opt/workspace\"

      requirements:
       - host: k8s_master_server
       - dependency: helm_install


    pub_network:
      type: tosca.nodes.network.Network
      properties:
        network_type: public

    server_pub_port:
      type: tosca.nodes.indigo.network.Port
      properties:
        order: 1
        dns_name: { concat: [ 'dydns:', get_input: dyn_dns_service_token, '@', get_input: frontend_dns_name  ] }
      requirements:
        - binding: k8s_master_server
        - link: pub_network

    priv_network:
      type: tosca.nodes.network.Network
      properties:
        network_type: private

    server_priv_port:
      type: tosca.nodes.network.Port
      properties:
        order: 0
      requirements:
        - binding: k8s_master_server
        - link: priv_network

    node_priv_port:
      type: tosca.nodes.network.Port
      properties:
        order: 0
      requirements:
        - binding: k8s_node_server
        - link: priv_network


    k8s_master:
      type: tosca.nodes.DODAS.FrontEnd.Kubernetes
      properties:
        kube_version: 1.23.8
        admin_token: { get_input: admin_token }
        kube_master_dns_name: { get_input: frontend_dns_name }
      requirements:
        - host: k8s_master_server

    k8s_node:
      type: tosca.nodes.DODAS.WorkerNode.Kubernetes
      properties:
        kube_version: 1.23.8
        front_end_ip: { get_attribute: [ k8s_master_server, private_address, 0 ] }
      requirements:
        - host: k8s_node_server

    k8s_master_server:
      type: tosca.nodes.indigo.Compute
      capabilities:
        endpoint:
          properties:
            ports: { get_input: ports }
        scalable:
          properties:
            count: { get_input: number_of_masters }
        host:
          properties:
            #instance_type:  m1.medium
            num_cpus: { get_input: num_cpus_master }
            mem_size: { get_input: mem_size_master }
        os:
          properties:
            distribution: ubuntu
            version: 20.04

    k8s_node_server:
      type: tosca.nodes.indigo.Compute
      capabilities:
        scalable:
          properties:
            count: { get_input: number_of_nodes }
        host:
          properties:
            #instance_type:  m1.large
            num_cpus: { get_input: num_cpus_node }
            mem_size: { get_input: mem_size_node }
        os:
          properties:
            distribution: ubuntu
            version: 20.04


  policies:
  - deploy_on_specific_site:
      type: tosca.policies.indigo.SlaPlacement
      properties:
        sla_id: eb7d2288-e190-4d54-9fff-ad67776740ba

  outputs:
    k8s_endpoint:
      value: { concat: [ 'https://', get_attribute: [ k8s_master_server, public_address, 0 ],  ':6443' ] }
    k8s_dashboard_endpoint:
      value: { concat: ['https://' , get_property: [ k8s_master, kube_master_dns_name], '/dashboard/']}
    grafana_endpoint:
      value: { concat: ['https://' , get_property: [ k8s_master, kube_master_dns_name], '/grafana']}
    jlab_endpoint:
      value: { concat: ['https://' , get_property: [ k8s_master, kube_master_dns_name], '/jlab/']}
    grafana_username:
      value: admin
    k8s_master_ip:
      value: { get_attribute: [ k8s_master_server, public_address, 0 ] }
    k8s_master_node_creds:
      value: { get_attribute: [ k8s_master_server, endpoint, credential, 0 ] }
    k8s_wn_ip:
      value: { get_attribute: [ k8s_node_server, private_address ] }
    k8s_wn_node_creds:
      value: { get_attribute: [ k8s_node_server, endpoint, credential ] }
